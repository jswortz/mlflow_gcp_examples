{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba3c3255-5191-47ea-925c-e34e1de598a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Training a simple Sklearn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation - RUN ONCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spark mlflow pyspark mlflow --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install java on machine\n",
    "# !wget https://download.oracle.com/java/19/latest/jdk-19_linux-x64_bin.deb\n",
    "# !sudo dpkg -i jdk-19_linux-x64_bin.deb\n",
    "# !echo Y | sudo apt-get install -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Set Javahome\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/jdk-19\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"]+\":\"+os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ca25e68-4b3c-48f4-9ddb-cfa92d64caa4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ab269cb-513a-453a-b0d9-9f2a7941b918",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# General\n",
    "import random\n",
    "import string\n",
    "\n",
    "# Preprocessing\n",
    "from pyspark import *\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "\n",
    "# Training\n",
    "import mlflow\n",
    "from pyspark.ml.feature import FeatureHasher, VectorAssembler\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cd4afae-277d-4df0-815c-3ad32ae93140",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "424d4713-c1b7-4692-bfa4-4b1f54477379",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "845f824a-e377-452f-804b-3807d01e0336",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a9ae5a2-2d37-4fdc-a5a0-226a489f630b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "UUID = generate_uuid(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a031383-0d4e-4191-a7b5-58a407755d73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data ingestion\n",
    "FILE_PATH = \"rawfiles/\"\n",
    "SALES_TABLE_NAME = f\"sales_data_set\"\n",
    "FEATURES_TABLE_NAME = f\"features_data_set\"\n",
    "STORES_TABLE_NAME = f\"stores_data_set\"\n",
    "\n",
    "FINAL_DATASET_SCHEMA_NAME = \"databricks_vertex\"\n",
    "FINAL_DATASET_TABLE_NAME = f\"kaggle_retail_dataset_{UUID}\"\n",
    "MODEL_NAME = f\"Weekly_Sales_GBTR_model_{UUID}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50c94117-5dfd-4803-b89c-5542a98b2457",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Ingest data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/jupyter/.local/lib/python3.7/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jupyter/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jupyter/.ivy2/jars\n",
      "org.mlflow#mlflow-spark added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-cc0f1c1f-aec2-4e82-94c5-0e7ee8fc8329;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mlflow#mlflow-spark;1.11.0 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.25 in central\n",
      ":: resolution report :: resolve 191ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\torg.mlflow#mlflow-spark;1.11.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.25 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-cc0f1c1f-aec2-4e82-94c5-0e7ee8fc8329\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/6ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/14 15:27:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/home/jupyter/.local/lib/python3.7/site-packages/pyspark/sql/context.py:114: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "### create spark context\n",
    "warehouse_location='hive/'\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"MLFLOW-TRAINING\").config(\"spark.jars.packages\", \"org.mlflow:mlflow-spark:1.11.0\")\\\n",
    ".config(\"spark.sql.warehouse.dir\", warehouse_location)\\\n",
    ".config(\"spark.sql.catalogImplementation\", \"hive\").getOrCreate()\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29f9970a-073d-4c89-af90-66902c16f87a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sales_df = spark.read.csv(FILE_PATH+SALES_TABLE_NAME+\".csv\", header=\"true\", inferSchema=\"true\")\n",
    "features_df = spark.read.csv(FILE_PATH+FEATURES_TABLE_NAME+\".csv\", header=\"true\", inferSchema=\"true\")\n",
    "stores_df = spark.read.csv(FILE_PATH+STORES_TABLE_NAME+\".csv\", header=\"true\", inferSchema=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8a1f012-5347-4210-bb10-77d6c6b63b01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/14 15:27:48 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/02/14 15:27:48 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/02/14 15:27:50 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "23/02/14 15:27:50 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore jupyter@10.128.0.44\n",
      "23/02/14 15:27:51 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "23/02/14 15:27:51 WARN ObjectStore: Failed to get database kaggle_retail_dataset_goq6, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_schema = 'CREATE SCHEMA IF NOT EXISTS {}'.format(FINAL_DATASET_SCHEMA_NAME)\n",
    "query_table = 'DROP TABLE IF EXISTS {0}.{1}'.format(FINAL_DATASET_TABLE_NAME,FINAL_DATASET_TABLE_NAME)\n",
    "sqlContext.sql(query_schema)\n",
    "sqlContext.sql(query_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef13822e-33fd-48a8-880c-0ab831e4d272",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales_df.createOrReplaceTempView(SALES_TABLE_NAME+UUID)\n",
    "features_df.createOrReplaceTempView(FEATURES_TABLE_NAME+UUID)\n",
    "stores_df.createOrReplaceTempView(STORES_TABLE_NAME+UUID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d45bbd30-fbfd-4f3c-add1-e2d87b1c5fc6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/14 15:28:15 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "`databricks_vertex`.`kaggle_retail_dataset_goq6` already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_29119/2006606565.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"CREATE TABLE {FINAL_DATASET_SCHEMA_NAME}.{FINAL_DATASET_TABLE_NAME} AS SELECT sa.Store, sa.Dept, CAST(LEFT(sa.Date, 2) AS int) AS day, CAST(SUBSTRING(sa.Date, 4, 2) AS int) AS month, CAST(RIGHT(sa.Date, 4) AS int) AS year, sa.Weekly_Sales, sa.IsHoliday, f.Temperature, f.Fuel_Price, f.MarkDown1, f.MarkDown2, f.MarkDown3, f.MarkDown4, f.MarkDown5, f.CPI, f.Unemployment, st.Type, CAST(st.Size AS decimal) AS Size FROM {SALES_TABLE_NAME+UUID} AS sa INNER JOIN {FEATURES_TABLE_NAME+UUID} AS f on (f.Store=sa.Store AND f.Date=sa.Date) INNER JOIN {STORES_TABLE_NAME+UUID} as st ON st.Store=sa.Store limit 10000\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pyspark/sql/context.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \"\"\"\n\u001b[0;32m--> 545\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0msqlQuery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: `databricks_vertex`.`kaggle_retail_dataset_goq6` already exists."
     ]
    }
   ],
   "source": [
    "query = f\"CREATE TABLE {FINAL_DATASET_SCHEMA_NAME}.{FINAL_DATASET_TABLE_NAME} AS SELECT sa.Store, sa.Dept, CAST(LEFT(sa.Date, 2) AS int) AS day, CAST(SUBSTRING(sa.Date, 4, 2) AS int) AS month, CAST(RIGHT(sa.Date, 4) AS int) AS year, sa.Weekly_Sales, sa.IsHoliday, f.Temperature, f.Fuel_Price, f.MarkDown1, f.MarkDown2, f.MarkDown3, f.MarkDown4, f.MarkDown5, f.CPI, f.Unemployment, st.Type, CAST(st.Size AS decimal) AS Size FROM {SALES_TABLE_NAME+UUID} AS sa INNER JOIN {FEATURES_TABLE_NAME+UUID} AS f on (f.Store=sa.Store AND f.Date=sa.Date) INNER JOIN {STORES_TABLE_NAME+UUID} as st ON st.Store=sa.Store limit 10000\"\n",
    "sqlContext.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d38053b-8f06-4770-83b2-8a0eece9a63c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "preprocessed_df = sqlContext.table(FINAL_DATASET_SCHEMA_NAME+\".\"+FINAL_DATASET_TABLE_NAME)\n",
    "preprocessed_df = preprocessed_df.withColumn(\"Size_double\", preprocessed_df['Size'].cast(\"double\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3c4fdcc-cc09-48ae-b8e4-10fd383781e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Training-Test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a2f98ea-4157-4a50-809c-f087be9f658e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(train, test) = preprocessed_df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8dffade5-72bf-4f93-a8aa-981786cc92a4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set tracking server\n",
    "mlflow.set_tracking_uri(\"http://34.123.222.224:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29f79adc-f167-48e8-bff1-17159b29e8ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/14 15:28:37 WARN DAGScheduler: Broadcasting large task binary with size 14.7 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/14 15:28:39 WARN DAGScheduler: Broadcasting large task binary with size 14.7 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/14 15:28:41 WARN DAGScheduler: Broadcasting large task binary with size 17.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "# enable MLFlow autologging\n",
    "mlflow.spark.autolog()\n",
    "\n",
    "# start MLFlow run\n",
    "with mlflow.start_run(run_name='Weekly_Sales_GBTR'+UUID) as run:\n",
    "    \n",
    "    # add hasher\n",
    "    hasher = FeatureHasher(inputCols=['IsHoliday', 'day', 'month', 'year', 'Temperature', 'Fuel_Price', 'Size_double', 'Store', 'Dept', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Type'],\n",
    "                       outputCol=\"features\")\n",
    "    \n",
    "    # add assembler\n",
    "    feat_cols=[\"features\"]\n",
    "    assembler = VectorAssembler(inputCols=feat_cols, outputCol=\"features_dense\")\n",
    "    \n",
    "    # model definition with parameters\n",
    "    gbtr = GBTRegressor(featuresCol='features_dense', labelCol='Weekly_Sales', maxIter=1)\n",
    "    \n",
    "    # define pipeline\n",
    "    pipeline = Pipeline(stages=[hasher, assembler, gbtr])\n",
    "    \n",
    "    # training pipeline\n",
    "    gbtr_model = pipeline.fit(train)\n",
    "    \n",
    "    # predictions on test data\n",
    "    gbtr_predictions = gbtr_model.transform(test)\n",
    "\n",
    "    # log model\n",
    "    mlflow.spark.log_model(gbtr_model, MODEL_NAME)\n",
    "  \n",
    "    # log the rmse\n",
    "    rmse=RegressionEvaluator(labelCol=\"Weekly_Sales\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse=rmse.evaluate(gbtr_predictions) \n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    \n",
    "    # log mae\n",
    "    mae=RegressionEvaluator(labelCol=\"Weekly_Sales\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "    mae=mae.evaluate(gbtr_predictions) \n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    \n",
    "    # log r^2\n",
    "    r2=RegressionEvaluator(labelCol=\"Weekly_Sales\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "    r2=r2.evaluate(gbtr_predictions)\n",
    "    mlflow.log_metric(\"R-Square\", r2)\n",
    "    \n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4038714276542663,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "training",
   "notebookOrigID": 2008756085747697,
   "widgets": {}
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m103"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
